% Encoding: UTF-8
@InProceedings{hug:nsdi16,
  Title                    = {{HUG}: Multi-Resource Fairness for Correlated and Elastic Demands},
  Author                   = {M. Chowdhury and Z. Liu and A. Ghodsi and I. Stoica},
  Booktitle                = {USENIX NSDI},
  Year                     = {2016},
  Month                    = {March},

  Abstract                 = {In this paper, we study how to optimally provide isolation guarantees in multi-resource environments, such as public clouds, where a tenant's demands on different resources (links) are correlated. Unlike prior work such as Dominant Resource Fairness (DRF) that assumes static and fixed demands, we consider elastic demands. Our approach generalizes canonical max-min fairness to the multi-resource setting with correlated demands, and extends DRF to elastic demands. We consider two natural optimization objectives: isolation guarantee from a tenant's viewpoint and system utilization (work conservation) from an operator's perspective. We prove that in non-cooperative environments like public cloud networks, there is a strong tradeoff between optimal isolation guarantee and work conservation when demands are elastic. Even worse, work conservation can even decrease network utilization instead of improving it when demands are inelastic. We identify the root cause behind the tradeoff and present a provably optimal allocation algorithm, High Utilization with Guarantees (HUG), to achieve maximum attainable network utilization without sacrificing the optimal isolation guarantee, strategy-proofness, and other useful properties of DRF. In cooperative environments like private datacenter networks, HUG achieves both the optimal isolation guarantee and work conservation. Analyses, simulations, and experiments show that HUG provides better isolation guarantees, higher system utilization, and better tenant-level performance than its counterparts.}
}

@InProceedings{carbyne:osdi16,
  Title                    = {Altruistic Scheduling in Multi-Resource Clusters},
  Author                   = {R. Grandl and M. Chowdhury and A. Akella and G. Ananthanarayanan},
  Booktitle                = {USENIX OSDI},
  Year                     = {2016},
  Month                    = {October},

  Abstract                 = {Given the well-known tradeoffs between fairness, performance, and efficiency, modern cluster schedulers often prefer instantaneous fairness as their primary objective to ensure performance isolation between users and groups. However, instantaneous, short-term convergence to fairness often does not result in noticeable long-term benefits. Instead, we propose an altruistic, long-term approach, Carbyne, where jobs yield fractions of their allocated resources without impacting their own completion times. We show that leftover resources collected via altruisms of many jobs can then be rescheduled to further secondary goals such as application-level performance and cluster efficiency without impacting performance isolation. Deployments and large-scale simulations show that Carbyne closely approximates the state-of-the-art solutions (e.g., DRF [27]) in terms of performance isolation, while providing 1.26X better efficiency and 1.59X lower average job completion time.}
}

@Article{infiniswap:login17,
  Title                    = {Decentralized Memory Disaggregation Over Low-Latency Networks},
  Author                   = {J. Gu and Y. Lee and Y. Zhang and M. Chowdhury and K. G. Shin},
  Journal                  = {USENIX ;login:},
  Year                     = {2017},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {42--48},
  Volume                   = {42},

  Abstract                 = {Memory disaggregation can expose remote memory across a cluster to local applications. However, existing proposals call for new architectures and/or new programming models, making them infeasible. We have developed a practical memory disaggregation solution, Infiniswap, which is a remote memory paging system for clusters with lowlatency, kernel-bypass networks such as RDMA. Infiniswap opportunistically harvests and transparently exposes unused memory across the cluster to unmodified applications by dividing the swap space of each machine into many chunks and distributing them to unused memory of many remote machines. For scalability, it leverages the power of many choices to perform decentralized memory chunk placements and evictions. Applications using Infiniswap receive large performance boosts when their working sets are larger than their physical memory allocations.}
}

@InProceedings{infiniswap:nsdi17,
  Title                    = {Efficient Memory Disaggregation with {Infiniswap}},
  Author                   = {J. Gu and Y. Lee and Y. Zhang and M. Chowdhury and K. G. Shin},
  Booktitle                = {USENIX NSDI},
  Year                     = {2017},
  Month                    = {March},

  Abstract                 = {Memory-intensive applications suffer large performance loss when their working sets do not fully fit in memory. Yet, they cannot leverage otherwise unused remote memory when paging out to disks even in the presence of large imbalance in memory utilizations across a cluster. Existing proposals for memory disaggregation call for new architectures, new hardware designs, and/or new programming models, making them infeasible.

This paper describes the design and implementation of Infiniswap, a remote memory paging system designed specifically for an RDMA network. Infiniswap opportunistically harvests and transparently exposes unused memory to unmodified applications by dividing the swap space of each machine into many slabs and distributing them across many machines' remote memory. Because one-sided RDMA operations bypass remote CPUs, Infiniswap leverages the power of many choices to perform decentralized slab placements and evictions.

We have implemented and deployed Infiniswap on an RDMA cluster without any modifications to user applications or the OS and evaluated its effectiveness using multiple workloads running on unmodified VoltDB, Memcached, PowerGraph, GraphX, and Apache Spark. Using Infiniswap, throughputs of these applications improve between 4X (0.94X) to 15.4X (7.8X) over disk (Mellanox nbdX), and median and tail latencies between 5.4X (2X) and 61X (2.3X). Infiniswap achieves these with negligible remote CPU usage, whereas nbdX becomes CPU-bound. Infiniswap increases the overall memory utilization of a cluster and works well at scale.}
}

@TechReport{cellscope:tr16,
  Title                    = {Fast and Accurate Performance Analysis of {LTE} Radio Access Networks},
  Author                   = {A. P. Iyer and I. Stoica and M. Chowdhury and L. E. Li},
  Institution              = {CoRR},
  Year                     = {2016},
  Month                    = {May},
  Number                   = {abs/1605.04652},

  Abstract                 = {An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy.

In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5x to 4.4x while reducing the model update overhead by up to 4.8x. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.}
}

@InProceedings{deepstack:hotos17,
  Title                    = {No! Not Another Deep Learning Framework},
  Author                   = {L. Nguyen and P. Yu and M. Chowdhury},
  Booktitle                = {ACM HotOS},
  Year                     = {2017},
  Month                    = {May},

  Abstract                 = {In recent years, deep learning has pervaded many areas of computing due to the confluence of an explosive growth of large-scale computing capabilities, availability of datasets, and advances in learning techniques. While this rapid growth has resulted in diverse deep learning frameworks, it has also led to inefficiencies for both the users and developers of these frameworks. Specifically, adopting useful techniques across frameworks -- both to perform learning tasks and to optimize performance -- involves significant repetitions and reinventions.

In this paper, we observe that despite their diverse origins, many of these frameworks share architectural similarities. We argue that by introducing a common representation of learning tasks and a hardware abstraction model to capture compute heterogeneity, we might be able to relieve machine learning researchers from dealing with low-level systems issues and systems researchers from being tied to any specific framework. We expect this decoupling to accelerate progress in both domains.}
}



@InProceedings{eccache:osdi16,
  Title                    = {{EC-Cache}: Load-Balanced, Low-Latency Cluster Caching with Online Erasure Coding},
  Author                   = {K. V. Rashmi and M. Chowdhury and J. Kosaian and I. Stoica and K. Ramchandran},
  Booktitle                = {USENIX OSDI},
  Year                     = {2016},
  Month                    = {October},

  Abstract                 = {Data-intensive clusters and object stores are increasingly relying on in-memory object caching to meet the I/O performance demands. These systems routinely face the challenges of popularity skew, background load imbalance, and server failures, which result in severe load imbalance across storage servers and degraded I/O performance. Selective replication is a commonly used technique to tackle these challenges, where the number of cached replicas of an object is proportional to its popularity. In this paper, we explore an alternative approach using erasure coding.

EC-Cache is a load-balanced, low latency cluster cache that uses online erasure coding to overcome the limitations of selective replication. EC-Cache employs erasure coding by: (i) splitting and erasure coding individual objects during writes, and (ii) late binding, wherein obtaining any k out of (k + r) splits of an object are sufficient, during reads. As compared to selective replication, EC-Cache improves load balancing by more than 3X and reduces the median and tail read latencies by more than 2X, while using the same amount of memory. EC-Cache does so using 10% additional bandwidth and a small increase in the amount of stored metadata. The benefits offered by EC-Cache are further amplified in the presence of background network load imbalance and server failures.}
}

@InProceedings{coda:sigcomm16,
  Title                    = {{CODA}: Toward Automatically Identifying and Scheduling {CO}flows in the {DA}rk},
  Author                   = {H. Zhang and L. Chen and B. Yi and K. Chen and M. Chowdhury and Y. Geng},
  Booktitle                = {ACM SIGCOMM},
  Year                     = {2016},
  Month                    = {August},

  Abstract                 = {Leveraging application-level requirements using coflows has recently been shown to improve application-level communication performance in data-parallel clusters. However, existing coflow-based solutions rely on modifying applications to extract coflows, making them inapplicable to many practical scenarios.

In this paper, we present CODA, a first attempt at automatically identifying and scheduling coflows without any application modifications. We employ an incremental clustering algorithm to perform fast, application-transparent coflow identification and complement it by proposing an error-tolerant coflow scheduler to mitigate occasional identification errors. Testbed experiments and large-scale simulations with production workloads show that CODA can identify coflows with over 90% accuracy, and its scheduler is robust to inaccuracies, enabling communication stages to complete 2.4X (5.1X) faster on average (95th percentile) compared to per-flow mechanisms. Overall, CODAï¿½s performance is comparable to that of solutions requiring application modifications.}
}

@InProceedings{hermes:sigcomm17,
  Title                    = {Resilient Datacenter Load Balancing in the Wild},
  Author                   = {H. Zhang and J. Zhang and W. Bai K. Chen and M. Chowdhury},
  Booktitle                = {ACM SIGCOMM},
  Year                     = {2017},
  Month                    = {August},

  Abstract                 = {Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance.

In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10% and 20% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.}
}

@InProceedings{fairdma:kbnets2017,
  Title                    = {Performance Isolation Anomalies in {RDMA}},
  Author                   = {Y. Zhang and J. Gu and Y. Lee and M. Chowdhury and K. G. Shin},
  Booktitle                = {ACM SIGCOMMKBNets},
  Year                     = {2017},
  Month                    = {August},

  Abstract                 = {To meet the increasing throughput and latency demands of modern applications, many operators are rapidly deploying RDMA in their datacenters. At the same time, developers are re-designing their software to take advantage of RDMA's benefits for individual applications. However, when it comes to RDMA's performance, many simple questions remain open.

In this paper, we consider the performance isolation characteristics of RDMA. Specifically, we conduct three sets of experiments -- three combinations of one throughput-sensitive flow and one latency-sensitive flow -- in a controlled environment, observe large discrepancies in RDMA performance with and without the presence of a competing flow, and describe our progress in identifying plausible root-causes.}
}

@InProceedings{pdd:apnet18,
  author    = {Hong Zhang and Kai Chen and Mosharaf Chowdhury},
  booktitle = {ACM APNet},
  title     = {Pas de deux: Shape the Circuits, and Shape the Apps too!},
  year      = {2018},
  pages     = {29--35},
}

@InProceedings{aga:grades-nda18,
  author    = {Anand Padmanabha Iyer and Aurojit Panda and Shivaram Venkataraman and Mosharaf Chowdhury and Aditya Akella and Scott Shenker and Ion Stoica},
  booktitle = {ACM SIGMOD GRADES-NDA},
  title     = {Bridging the {GAP}: Towards Approximate Graph analytics},
  year      = {2018},
}

@InProceedings{monarch:hotcloud18,
  author    = {Anand Padmanabha Iyer and Aurojit Panda and Mosharaf Chowdhury and Aditya Akella and Scott Shenker and Ion Stoica},
  booktitle = {USENIX HotCloud},
  title     = {Monarch: Gaining Command on Geo-Distributed Graph Analytics},
  year      = {2018},
}

@InProceedings{relay:hotcloud18,
  author    = {Fan Lai and Mosharaf Chowdhury and Harsha V. Madhyastha},
  booktitle = {USENIX HotCloud},
  title     = {To Relay or Not to Relay for Inter-Cloud Transfers?},
  year      = {2018},
}

@InProceedings{cellscope:mobicom18,
  author    = {Anand Padmanabha Iyer and Li Erran Li and Mosharaf Chowdhury and Ion Stoica},
  booktitle = {ACM MobiCom},
  title     = {Mitigating the Latency-Accuracy Trade-off in Mobile Data Analytics Systems},
  year      = {2018},
  pages     = {513--528},
}

@InProceedings{qoop:osdi18,
  author    = {Kshiteej Mahajan and Mosharaf Chowdhury and Aditya Akella and Shuchi Chawla},
  booktitle = {USENIX OSDI},
  title     = {Dynamic Query Re-Planning using {QOOP}},
  year      = {2018},
  pages     = {253--267},
}

@InProceedings{dslr:sigmod18,
  author    = {Dong Young Yoon and Mosharaf Chowdhury and Barzan Mozafari},
  booktitle = {ACM SIGMOD},
  title     = {Distributed Lock Management with {RDMA}: Decentralization without Starvation},
  year      = {2018},
  pages     = {1571--1586},
}

@InProceedings{allox:mama18,
  author    = {Xiao Sun and Tan N. Le and Mosharaf Chowdhury and Zhenhua Liu},
  booktitle = {ACM SIGMETRICS MAMA},
  title     = {Fair Allocation of Heterogeneous and Interchangeable Resources},
  year      = {2018},
}

@InProceedings{tiresias:nsdi19,
  author    = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Harry Liu and Chuanxiong Guo},
  booktitle = {USENIX NSDI},
  title     = {Tiresias: {A} {GPU} Cluster Manager for Distributed Deep Learning},
  year      = {2019},
  pages     = {485--500},
}

@InProceedings{nocs:spaa19,
  author    = {Mosharaf Chowdhury and Samir Khuller and Manish Purohit and Sheng Yang and Jie You},
  booktitle = {ACM SPAA},
  title     = {Near Optimal Coflow Scheduling in Networks},
  year      = {2019},
  pages     = {123--134},
}

@Article{salus:arxiv19,
  author        = {Peifeng Yu and Mosharaf Chowdhury},
  journal       = {CoRR},
  title         = {Salus: Fine-Grained {GPU} Sharing Primitives for Deep Learning Applications},
  year          = {2019},
  volume        = {abs/1902.04610},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1902-04610.bib},
  eprint        = {1902.04610},
  url           = {http://arxiv.org/abs/1902.04610},
}

@Article{terra:arxiv19,
  author        = {Jie You and Mosharaf Chowdhury},
  journal       = {CoRR},
  title         = {Terra: Scalable Cross-Layer {GDA} Optimizations},
  year          = {2019},
  volume        = {abs/1904.08480},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1904-08480.bib},
  eprint        = {1904.08480},
  url           = {http://arxiv.org/abs/1904.08480},
}

@Article{justitia:arxiv19,
  author        = {Yiwen Zhang and Yue Tan and Brent Stephens and Mosharaf Chowdhury},
  journal       = {CoRR},
  title         = {{RDMA} Performance Isolation With Justitia},
  year          = {2019},
  volume        = {abs/1905.04437},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1905-04437.bib},
  eprint        = {1905.04437},
  url           = {http://arxiv.org/abs/1905.04437},
}

@Article{nocs:arxiv19,
  author        = {Mosharaf Chowdhury and Samir Khuller and Manish Purohit and Sheng Yang and Jie You},
  journal       = {CoRR},
  title         = {Near Optimal Coflow Scheduling in Networks},
  year          = {2019},
  volume        = {abs/1906.06851},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1906-06851.bib},
  eprint        = {1906.06851},
  url           = {http://arxiv.org/abs/1906.06851},
}

@Article{hydra:arxiv19,
  author        = {Youngmoon Lee and Hassan Al Maruf and Mosharaf Chowdhury and Kang G. Shin},
  journal       = {CoRR},
  title         = {Mitigating the Performance-Efficiency Tradeoff in Resilient Memory Disaggregation},
  year          = {2019},
  volume        = {abs/1910.09727},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1910-09727.bib},
  eprint        = {1910.09727},
  url           = {http://arxiv.org/abs/1910.09727},
}

@Article{leap:arxiv19,
  author        = {Hasan Al Maruf and Mosharaf Chowdhury},
  journal       = {CoRR},
  title         = {Effectively Prefetching Remote Memory with Leap},
  year          = {2019},
  volume        = {abs/1911.09829},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1911-09829.bib},
  eprint        = {1911.09829},
  url           = {http://arxiv.org/abs/1911.09829},
}

@Article{bopf:arxiv19,
  author        = {Tan N. Le and Xiao Sun and Mosharaf Chowdhury and Zhenhua Liu},
  journal       = {CoRR},
  title         = {BoPF: Mitigating the Burstiness-Fairness Tradeoff in Multi-Resource Clusters},
  year          = {2019},
  volume        = {abs/1912.03523},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1912-03523.bib},
  eprint        = {1912.03523},
  url           = {http://arxiv.org/abs/1912.03523},
}

@InProceedings{allox:eurosys20,
  author    = {Tan N. Le and Xiao Sun and Mosharaf Chowdhury and Zhenhua Liu},
  booktitle = {ACM EuroSys},
  title     = {AlloX: compute allocation in hybrid clusters},
  year      = {2020},
  pages     = {31:1--31:16},
}

@InProceedings{salus:mlsys20,
  author    = {Peifeng Yu and Mosharaf Chowdhury},
  booktitle = {MLSys},
  title     = {Fine-Grained {GPU} Sharing Primitives for Deep Learning Applications},
  year      = {2020},

  publist_confkey = {MLSys'20},
  publist_link = {paper || salus-mlsys20.pdf},
  publist_link = {talk || salus-mlsys20-talk.pptx},
  publist_link = {poster || salus-mlsys20-poster.pdf},
  publist_badge = {Artifacts Available},
  publist_badge = {Artifacts Evaluated Functional},
  publist_badge = {Artifacts Replicated},
  publist_abstract = {
    Unlike traditional resources such as CPU or the network, modern GPUs do not natively support
    fine-grained sharing primitives.
    Consequently, implementing common policies such as time sharing and preemption are expensive. Worse,
    when a deep learning (DL) application cannot completely use a GPU's resources, the GPU cannot be efficiently shared
    between multiple applications, leading to GPU underutilization.

    We present Salus to enable two GPU sharing primitives: __fast job
    switching__ and __memory sharing__, to achieve fine-grained GPU sharing
    among multiple DL applications. Salus is an efficient, consolidated
    execution service that exposes the GPU to different DL applications, and it
    enforces fine-grained sharing by performing iteration scheduling and
    addressing associated memory management issues. We show that these primitives
    can then be used to implement flexible sharing policies. Our integration of
    Salus with TensorFlow and evaluation on popular DL jobs shows that Salus
    can improve the average completion time of DL training jobs by $3.19\times$, GPU utilization for
    hyper-parameter tuning by $2.38\times$, and GPU utilization of DL inference applications by $42\times$ over not sharing
    the GPU and $6\times$ over NVIDIA MPS with small overhead.
  }
}

@InProceedings{sol:nsdi20,
  author    = {Fan Lai and Jie You and Xiangfeng Zhu and Harsha V. Madhyastha and Mosharaf Chowdhury},
  booktitle = {USENIX NSDI},
  title     = {Sol: Fast Distributed Computation Over Slow Networks},
  year      = {2020},
  pages     = {273--288},
}

@InProceedings{pando:nsdi20,
  author    = {Muhammed Uluyol and Anthony Huang and Ayush Goel and Mosharaf Chowdhury and Harsha V. Madhyastha},
  booktitle = {USENIX NSDI},
  title     = {Near-Optimal Latency Versus Cost Tradeoffs in Geo-Distributed Storage},
  year      = {2020},
  pages     = {157--180},
}

@InProceedings{netlock:sigcomm20,
  author    = {Zhuolong Yu and Yiwen Zhang and Vladimir Braverman and Mosharaf Chowdhury and Xin Jin},
  booktitle = {ACM SIGCOMM},
  title     = {NetLock: Fast, Centralized Lock Management Using Programmable Switches},
  year      = {2020},
  pages     = {126--138},
}

@InProceedings{leap:atc20,
  author    = {Hasan Al Maruf and Mosharaf Chowdhury},
  booktitle = {USENIX ATC},
  title     = {Effectively Prefetching Remote Memory with Leap},
  year      = {2020},
  pages     = {843--857},
}
